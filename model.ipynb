{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import suncalc\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YinYingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): 数据集的根目录路径。\n",
    "            transform (callable, optional): 需要应用于样本的可选变换。\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.feature= []\n",
    "        self.label = []\n",
    "        self.angle = []\n",
    "        self._load_dataset_path()\n",
    "\n",
    "    def _load_dataset_path(self):\n",
    "        \"\"\"\n",
    "        遍历数据集目录，加载所有数据和标签的路径。\n",
    "        \"\"\"\n",
    "        for class_folder in os.listdir(self.root_dir):\n",
    "            class_folder_path = os.path.join(self.root_dir, class_folder)\n",
    "            for sub_class_folder in os.listdir(class_folder_path):\n",
    "                sub_class_folder_path = os.path.join(class_folder_path,sub_class_folder)\n",
    "                feature_path = os.path.join(sub_class_folder_path,'train','train.pkl')\n",
    "                label_path = os.path.join(sub_class_folder_path,'label','label.pkl')\n",
    "                angle_path = os.path.join(sub_class_folder_path,'train','position.pkl')\n",
    "                self.feature.append(feature_path)\n",
    "                self.label.append(label_path)\n",
    "                self.angle.append(angle_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_path = self.feature[idx]\n",
    "        label_path = self.label[idx]\n",
    "        angle_path = self.angle[idx]\n",
    "        feature = None \n",
    "        label = None \n",
    "        angle = None \n",
    "        with open(feature_path,'rb') as f:\n",
    "            feature = pickle.load(f)\n",
    "        with open(label_path,'rb') as f:\n",
    "            label = pickle.load(f)\n",
    "        with open(angle_path,'rb') as f:\n",
    "            angle = pickle.load(f)\n",
    "        feature = torch.Tensor(feature).squeeze(0)\n",
    "        label = torch.Tensor(label)\n",
    "        pad_h = 1280- feature.shape[0]\n",
    "        pad_w = 1280 - feature.shape[1]\n",
    "        pad_h = 1280- feature.shape[0]\n",
    "        pad_w = 1280 - feature.shape[1]\n",
    "        feature = F.pad(feature,(int(pad_w/2),pad_w-int(pad_w/2),int(pad_h/2),pad_h-int(pad_h/2)))\n",
    "        label = F.pad(label,(int(pad_w/2),pad_w-int(pad_w/2),int(pad_h/2),pad_h-int(pad_h/2)))\n",
    "        angle = torch.Tensor(angle)\n",
    "        mask = (feature != 0).float()        \n",
    "        return feature.unsqueeze(0), label, angle, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    '''\n",
    "    1D Image to Patch Embedding\n",
    "    '''\n",
    "    def __init__(self,img_size=1280,patch_size=20, in_c=1,embed_dim=400,norm_lay=None):\n",
    "        super().__init__()\n",
    "        # 图片分辨率\n",
    "        img_size = (img_size, img_size)\n",
    "        # 卷积核大小\n",
    "        patch_size = (patch_size,patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        # 分别计算w，h方向上的patch个数\n",
    "        self.grid_size = (img_size[0]//patch_size[0],img_size[1]//patch_size[1])\n",
    "        # 一张图片的patch个数\n",
    "        self.num_patches = self.grid_size[0]*self.grid_size[1]\n",
    "        self.embed_dim = self.patch_size[0]*self.patch_size[1]*in_c\n",
    "        # 卷积的步长实现图片切分操作，而后与patch大小一致的卷积核完成线性映射\n",
    "        self.proj = nn.Conv2d(in_c,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
    "        self.norm = norm_lay(embed_dim) if norm_lay else nn.Identity()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.proj(x).flatten(2).transpose(1,2)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "    \n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        return self.attn(x, x, x, need_weights=False)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Angle_Encoder(nn.Module):\n",
    "    def __init__(self, angle_length, emb_dim, dropout_rate=0.1):\n",
    "        super(Angle_Encoder,self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lr = nn.Linear(angle_length,emb_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1,4096,emb_dim))\n",
    "        self.transformer = Transformer(emb_dim,8,8)\n",
    "        if dropout_rate>0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "    def forward(self, x):\n",
    "        out = self.lr(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = out.repeat((1,4096,1))\n",
    "        out = out + self.pos_embedding\n",
    "        out = self.transformer(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        return out  \n",
    "\n",
    "class PositionEmbs(nn.Module):\n",
    "    def __init__(self, angle_length, num_patches, emb_dim, dropout_rate=0.1):\n",
    "        super(PositionEmbs,self).__init__()\n",
    "        self.angle_encoder = Angle_Encoder(angle_length, emb_dim,dropout_rate)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1,num_patches,emb_dim))\n",
    "        if dropout_rate >0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "    def forward(self,x, angle):\n",
    "        out = x+self.pos_embedding + self.angle_encoder(angle)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,   # 输入token的dim\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # (shape[0],1,1,1)\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)   # rand范围在[0~1]之间, +keep_prob在[keep_prob~keep_prob+1]之间\n",
    "    random_tensor.floor_()  # 只保留0或者1\n",
    "    output = x.div(keep_prob) * random_tensor   # x.div(keep_prob)个人理解是为了强化保留部分的x\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))   \n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, angle_length = 2, img_size=1280, patch_size=20, in_c=1, \n",
    "                 embed_dim=400, depth=12, num_heads=8, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer,self).__init__()\n",
    "        # embed_dim默认tansformer的base 256\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        # 源码distilled是为了其他任务,分类暂时不考虑\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        # LayerNorm:对每单个batch进行的归一化\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm,eps=1e-6)\n",
    "        # act_layer默认tansformer的GELU\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        # embed_layer默认是patch embedding,在其他应用中应该会有其他选择\n",
    "        # patch embedding过程\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        # patche个数\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # positional embedding过程\n",
    "        self.pos_embedding = PositionEmbs(angle_length, num_patches, embed_dim, drop_ratio)\n",
    "        # depth是Block的个数\n",
    "        # 不同block层数 drop_ratio的概率不同,越深度越高\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
    "        # blocks搭建\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "    def forward_features(self,x, angle):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_embedding(x, angle)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x, angle):\n",
    "        x = self.forward_features(x, angle)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义损失函数，仅对 mask 区域进行计算\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, label, mask):\n",
    "        mask = mask.float()\n",
    "        squared_loss = ((output - label) ** 2) * mask\n",
    "        mask_sum = mask.sum()\n",
    "\n",
    "        # 防止分母为零\n",
    "        if mask_sum == 0:\n",
    "            return torch.tensor(0.0, device=output.device)\n",
    "\n",
    "        loss = squared_loss.sum() / mask_sum\n",
    "        loss = torch.sqrt(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/'\n",
    "dataset = YinYingDataset(root_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# 计算测试集和训练集的大小\n",
    "dataset_size = len(dataset)\n",
    "test_size = int(dataset_size * 0.2)\n",
    "train_size = dataset_size - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  \n",
    "# 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 实例化模型、损失函数和优化器\n",
    "model =VisionTransformer().to(device)\n",
    "criterion = MaskedMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_list = []\n",
    "num_epochs = 20\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, (feature, label,angle, mask) in enumerate(train_loader):\n",
    "#         feature = feature.to(device)\n",
    "#         angle = angle.to(device)\n",
    "#         mask = mask.flatten().to(device)\n",
    "#         label = label.flatten().to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         pred = model(feature,angle).flatten()\n",
    "#         loss = criterion(pred, label, mask)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() \n",
    "#     epoch_loss = running_loss / len(dataloader.dataset)\n",
    "#     loss_list.append(epoch_loss)\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
